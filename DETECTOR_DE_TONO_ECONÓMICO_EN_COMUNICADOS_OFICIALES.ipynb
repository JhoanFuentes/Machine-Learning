{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpXAsgpaBzYia5hogoO18k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JhoanFuentes/Machine-Learning/blob/main/DETECTOR_DE_TONO_ECON%C3%93MICO_EN_COMUNICADOS_OFICIALES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IctFGBnP9AxX"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# ==============================================================================\n",
        "# PROYECTO: DETECTOR DE TONO ECONÓMICO EN COMUNICADOS OFICIALES\n",
        "# Implementación basada en la exploración del artículo de M. Dell (DL for Economists)\n",
        "# Compara Fine-tuning de un Transformer vs. Clasificación con GenAI (GPT)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# **PARTE 0: CONFIGURACIÓN INICIAL E INSTALACIONES**\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"=== PARTE 0: Configuración Inicial ===\")\n",
        "\n",
        "# Instalación de librerías necesarias en Google Colab\n",
        "!pip install transformers[torch] datasets evaluate scikit-learn openai pandas -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import evaluate  # Librería de Hugging Face para métricas\n",
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import openai # Para la parte de GenAI\n",
        "import os\n",
        "import getpass # Para pedir la API key de forma segura\n",
        "\n",
        "# Verificar si hay GPU disponible (recomendado para fine-tuning)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"Nombre de la GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"\\nInstalación y carga de librerías completada.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# **PARTE 1: PREPARACIÓN DE DATOS (¡SIMULADOS!)**\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n=== PARTE 1: Preparación de Datos (Simulados) ===\")\n",
        "\n",
        "# --- ¡¡¡IMPORTANTE!!! ---\n",
        "# Estos datos son SIMULADOS y muy pequeños. En un proyecto real, aquí cargarías\n",
        "# tus datos reales, preprocesados y etiquetados manualmente.\n",
        "# La calidad y cantidad de tus datos etiquetados es CRUCIAL (Ver Sec 4 del texto).\n",
        "\n",
        "simulated_data = [\n",
        "    # --- Optimista ---\n",
        "    {\"text\": \"La economía muestra signos robustos de recuperación, superando las expectativas del mercado.\", \"label_text\": \"Optimista\"},\n",
        "    {\"text\": \"Se proyecta un crecimiento significativo del PIB para el próximo trimestre gracias a la inversión extranjera.\", \"label_text\": \"Optimista\"},\n",
        "    {\"text\": \"El consumo interno ha repuntado notablemente, impulsando la actividad comercial.\", \"label_text\": \"Optimista\"},\n",
        "    {\"text\": \"Las exportaciones alcanzaron un nuevo récord histórico, fortaleciendo nuestra balanza comercial.\", \"label_text\": \"Optimista\"},\n",
        "\n",
        "    # --- Pesimista ---\n",
        "    {\"text\": \"Persisten presiones inflacionarias que podrían afectar el poder adquisitivo de los hogares.\", \"label_text\": \"Pesimista\"},\n",
        "    {\"text\": \"La incertidumbre global sigue representando un riesgo considerable para la inversión local.\", \"label_text\": \"Pesimista\"},\n",
        "    {\"text\": \"El mercado laboral aún no recupera los niveles pre-pandemia, mostrando debilidad.\", \"label_text\": \"Pesimista\"},\n",
        "    {\"text\": \"Se observa una desaceleración en el sector industrial que requiere atención.\", \"label_text\": \"Pesimista\"},\n",
        "\n",
        "    # --- Neutral ---\n",
        "    {\"text\": \"El comité de política monetaria decidió mantener la tasa de interés de referencia.\", \"label_text\": \"Neutral\"},\n",
        "    {\"text\": \"Se publicaron los datos actualizados del índice de precios al consumidor.\", \"label_text\": \"Neutral\"},\n",
        "    {\"text\": \"El tipo de cambio fluctuó dentro de los rangos esperados durante la última semana.\", \"label_text\": \"Neutral\"},\n",
        "    {\"text\": \"El análisis de los agregados monetarios se presentará en el próximo informe trimestral.\", \"label_text\": \"Neutral\"},\n",
        "\n",
        "    # --- Fáctico/Técnico (Podría fusionarse con Neutral o ser una clase aparte) ---\n",
        "    {\"text\": \"La metodología de cálculo del indicador X se ajustó según la normativa internacional.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"El modelo econométrico utilizado incorpora variables endógenas y exógenas específicas.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"Los resultados de la encuesta de opinión empresarial indican una variación interanual del 2.5%.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"Se aplicó un filtro Hodrick-Prescott para descomponer la serie de tiempo del producto.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"El coeficiente de Gini se ubicó en 0.52 para el último periodo medido.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"La deuda pública como porcentaje del PIB se mantuvo estable en el 45%.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"La inversión fija bruta experimentó un crecimiento trimestral desestacionalizado del 1.2%.\", \"label_text\": \"Fáctico\"},\n",
        "    {\"text\": \"El balance fiscal primario presentó un superávit equivalente al 0.8% del PIB.\", \"label_text\": \"Fáctico\"}\n",
        "]\n",
        "\n",
        "# Convertir a DataFrame de Pandas para facilidad\n",
        "df = pd.DataFrame(simulated_data)\n",
        "\n",
        "# Mapeo de etiquetas de texto a números (requerido por muchos modelos)\n",
        "labels = df['label_text'].unique().tolist()\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "id2label = {i: label for i, label in enumerate(labels)}\n",
        "df['label'] = df['label_text'].map(label2id)\n",
        "\n",
        "print(f\"\\nEtiquetas encontradas: {labels}\")\n",
        "print(f\"Mapeo label -> id: {label2id}\")\n",
        "print(f\"Mapeo id -> label: {id2label}\")\n",
        "print(f\"\\nNúmero de ejemplos simulados: {len(df)}\")\n",
        "print(f\"Distribución de etiquetas simuladas:\\n{df['label_text'].value_counts()}\")\n",
        "\n",
        "# Convertir el DataFrame a un Dataset de Hugging Face\n",
        "hg_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Crear las características (features) incluyendo la ClassLabel\n",
        "# Esto ayuda a Hugging Face a entender las etiquetas\n",
        "features = hg_dataset.features\n",
        "features['label'] = ClassLabel(names=labels)\n",
        "hg_dataset = hg_dataset.cast(features)\n",
        "\n",
        "# Dividir el dataset en entrenamiento, validación y prueba (80/10/10 split)\n",
        "# ¡Con tan pocos datos, esta división es solo ilustrativa! En la práctica,\n",
        "# necesitas cientos o miles de ejemplos etiquetados.\n",
        "train_testvalid = hg_dataset.train_test_split(test_size=0.3, seed=42, stratify_by_column=\"label\")\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42, stratify_by_column=\"label\")\n",
        "\n",
        "# Crear el DatasetDict final\n",
        "split_datasets = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'validation': test_valid['test'], # Usado para ajustar hiperparámetros y early stopping\n",
        "    'test': test_valid['train']       # Usado SOLO para la evaluación final\n",
        "})\n",
        "\n",
        "print(f\"\\nDataset dividido:\")\n",
        "print(split_datasets)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# **PARTE 2: FINE-TUNING DE UN CLASIFICADOR TRANSFORMER**\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n=== PARTE 2: Fine-tuning de Clasificador Transformer ===\")\n",
        "print(\"Esto puede tardar unos minutos, incluso con GPU y datos pequeños...\")\n",
        "\n",
        "# --- 2.1 Cargar Tokenizer y Modelo Pre-entrenado ---\n",
        "# Usaremos un modelo BERT pre-entrenado para español.\n",
        "# 'dccuchile/bert-base-spanish-wwm-uncased' es una opción popular.\n",
        "model_checkpoint = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Cargar el modelo pre-entrenado con una cabeza de clasificación encima.\n",
        "# Especificamos el número de etiquetas y los mapeos id<->label.\n",
        "model_finetuned = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device) # Mover el modelo a la GPU si está disponible\n",
        "\n",
        "print(f\"\\nTokenizer y Modelo '{model_checkpoint}' cargados.\")\n",
        "\n",
        "# --- 2.2 Preprocesamiento (Tokenización) ---\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniza los textos. `truncation=True` corta textos largos,\n",
        "    # `padding=True` (o 'max_length') rellena textos cortos.\n",
        "    # Max length puede ajustarse según tu análisis de longitud de textos.\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "# Aplicar la tokenización a todos los splits del dataset\n",
        "tokenized_datasets = split_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Eliminar columnas innecesarias después de tokenizar\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label_text\", \"__index_level_0__\"])\n",
        "# Renombrar 'label' a 'labels' que es el nombre esperado por el Trainer\n",
        "#tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "# Establecer el formato a tensores de PyTorch\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nDatasets tokenizados y formateados para PyTorch.\")\n",
        "print(tokenized_datasets)\n",
        "\n",
        "# Data collator se encarga de crear los lotes (batches) de datos\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# --- 2.3 Definir Métricas de Evaluación ---\n",
        "# Usaremos Accuracy y F1-score (Weighted para manejar posible desbalance)\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1_weighted = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    f1_macro = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\") # Macro es sensible al rendimiento en clases pequeñas\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "        \"f1_weighted\": f1_weighted[\"f1\"],\n",
        "        \"f1_macro\": f1_macro[\"f1\"],\n",
        "    }\n",
        "\n",
        "print(\"\\nFunción para calcular métricas definida.\")\n",
        "\n",
        "# --- 2.4 Configurar Argumentos de Entrenamiento ---\n",
        "# Estos son hiperparámetros clave. Necesitan ajuste (usando el validation set)\n",
        "# en un proyecto real. Los valores aquí son solo un ejemplo.\n",
        "# (Ver Sec 3 sobre optimización en el texto de Dell)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_finetuned\",          # Directorio para guardar el modelo y logs\n",
        "    num_train_epochs=5,                     # Número de épocas (ajustar según convergencia) - pocas por los pocos datos\n",
        "    per_device_train_batch_size=4,          # Tamaño del lote para entrenamiento (ajustar según memoria GPU)\n",
        "    per_device_eval_batch_size=8,           # Tamaño del lote para evaluación\n",
        "    warmup_steps=50,                        # Pasos de calentamiento para el learning rate scheduler\n",
        "    weight_decay=0.01,                      # Regularización L2\n",
        "    logging_dir='./logs_finetuned',             # Directorio para logs de TensorBoard\n",
        "    logging_steps=10,                       # Frecuencia de loggeo\n",
        "    evaluation_strategy=\"epoch\",            # Evaluar al final de cada época\n",
        "    save_strategy=\"epoch\",                  # Guardar el modelo al final de cada época\n",
        "    load_best_model_at_end=True,            # Cargar el mejor modelo encontrado durante el entrenamiento al final\n",
        "    metric_for_best_model=\"f1_macro\",       # Métrica para decidir cuál es el \"mejor\" modelo (F1 macro suele ser bueno para clases desbalanceadas)\n",
        "    greater_is_better=True,                 # La métrica elegida debe maximizarse\n",
        "    report_to=\"none\",                       # Deshabilitar reportes a WandB/Tensorboard por simplicidad aquí\n",
        "    fp16=torch.cuda.is_available(),         # Usar precisión mixta si hay GPU (acelera y ahorra memoria)\n",
        ")\n",
        "\n",
        "print(\"\\nArgumentos de entrenamiento configurados.\")\n",
        "\n",
        "# --- 2.5 Entrenar el Modelo ---\n",
        "# Crear el objeto Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_finetuned,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"], # ¡Usa validation set para evaluar durante el entrenamiento!\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# ¡Iniciar el entrenamiento!\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Guardar métricas de entrenamiento\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state() # Guarda el estado del trainer\n",
        "trainer.save_model(\"./results_finetuned/best_model\") # Guarda el mejor modelo explícitamente\n",
        "\n",
        "print(\"\\nEntrenamiento completado.\")\n",
        "\n",
        "# --- 2.6 Evaluar en el Conjunto de Prueba ---\n",
        "# ¡MUY IMPORTANTE! Evaluar el rendimiento final en el test set que\n",
        "# el modelo NUNCA vio durante el entrenamiento o la selección de hiperparámetros.\n",
        "print(\"\\nEvaluando el modelo fine-tuned en el CONJUNTO DE PRUEBA...\")\n",
        "test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "\n",
        "print(\"\\nMétricas en el Conjunto de Prueba (Fine-tuned):\")\n",
        "for key, value in test_metrics.items():\n",
        "    # Filtrar métricas relevantes de la evaluación del test set\n",
        "    if key in [\"eval_loss\", \"eval_accuracy\", \"eval_f1_weighted\", \"eval_f1_macro\", \"eval_runtime\", \"eval_samples_per_second\"]:\n",
        "        print(f\"  {key.replace('eval_', '')}: {value:.4f}\")\n",
        "\n",
        "trainer.log_metrics(\"test\", test_metrics)\n",
        "trainer.save_metrics(\"test\", test_metrics)\n",
        "\n",
        "\n",
        "# --- 2.7 Hacer Predicciones con el Modelo Fine-tuned ---\n",
        "def predict_tone_finetuned(text):\n",
        "    \"\"\"Función para predecir el tono de un nuevo texto usando el modelo fine-tuned.\"\"\"\n",
        "    # Tokenizar el texto de entrada\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "    # Hacer la predicción (desactivar cálculo de gradientes para inferencia)\n",
        "    with torch.no_grad():\n",
        "        logits = model_finetuned(**inputs).logits\n",
        "    # Obtener la clase predicha (el índice con el logit más alto)\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    # Devolver la etiqueta de texto correspondiente\n",
        "    return id2label[predicted_class_id]\n",
        "\n",
        "print(\"\\nProbando predicción con el modelo fine-tuned:\")\n",
        "test_sentence_1 = \"El panorama económico parece mejorar lentamente.\"\n",
        "test_sentence_2 = \"Las cifras fiscales muestran un déficit preocupante.\"\n",
        "print(f\"Texto: '{test_sentence_1}' -> Tono Predicho: {predict_tone_finetuned(test_sentence_1)}\")\n",
        "print(f\"Texto: '{test_sentence_2}' -> Tono Predicho: {predict_tone_finetuned(test_sentence_2)}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# **PARTE 3: CLASIFICACIÓN USANDO GENAI (EJEMPLO CON OPENAI GPT)**\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n=== PARTE 3: Clasificación usando GenAI (GPT) ===\")\n",
        "\n",
        "# --- ¡¡¡ADVERTENCIA!!! ---\n",
        "# Esta parte requiere una API Key de OpenAI.\n",
        "# El uso de la API de OpenAI tiene COSTOS asociados. Revisa sus precios.\n",
        "# NUNCA pongas tu API key directamente en el código. Usa Colab Secrets o variables de entorno.\n",
        "\n",
        "# Intentar obtener la API key de forma segura\n",
        "try:\n",
        "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    if not openai_api_key:\n",
        "         openai_api_key = getpass.getpass(\"Ingresa tu OpenAI API Key: \")\n",
        "    openai.api_key = openai_api_key\n",
        "    # Hacer una llamada de prueba simple para verificar la key (opcional, pero útil)\n",
        "    client = openai.OpenAI(api_key=openai_api_key)\n",
        "    client.models.list()\n",
        "    print(\"API Key de OpenAI configurada correctamente.\")\n",
        "    use_openai = True\n",
        "except Exception as e:\n",
        "    print(f\"Error al configurar la API de OpenAI: {e}\")\n",
        "    print(\"No se pudo configurar la API Key de OpenAI. Saltando la Parte 3.\")\n",
        "    use_openai = False\n",
        "\n",
        "if use_openai:\n",
        "    # --- 3.1 Definir la Función de Clasificación con GPT ---\n",
        "    # El modelo 'gpt-3.5-turbo' es una opción económica y rápida.\n",
        "    # 'gpt-4' o 'gpt-4o' son más potentes pero más caros.\n",
        "    # (Ver Sec 7 sobre experiencias de Dell con diferentes modelos GPT/Claude)\n",
        "    GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "    def classify_with_gpt(text_to_classify):\n",
        "        \"\"\"Clasifica el texto usando la API de OpenAI con un prompt específico.\"\"\"\n",
        "        # Prompt Engineering: Clave para el rendimiento de GenAI (Sec 7)\n",
        "        # - Ser claro y específico.\n",
        "        # - Pedir un formato de salida simple (solo la etiqueta).\n",
        "        # - Usar un conjunto de validación para probar y mejorar prompts (¡no el test set!)\n",
        "        system_prompt = f\"Eres un asistente experto en análisis económico. Tu tarea es clasificar el tono del siguiente texto. Las posibles categorías son: {', '.join(labels)}. Responde ÚNICAMENTE con UNA de esas categorías.\"\n",
        "        user_prompt = f\"Clasifica el tono del siguiente texto:\\n\\n\\\"\\\"\\\"\\n{text_to_classify}\\n\\\"\\\"\\\"\\n\\nTono:\"\n",
        "\n",
        "        try:\n",
        "            client = openai.OpenAI(api_key=openai_api_key)\n",
        "            response = client.chat.completions.create(\n",
        "                model=GPT_MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                temperature=0.0, # Baja temperatura para respuestas más deterministas/consistentes\n",
        "                max_tokens=10,    # Suficiente para una etiqueta corta\n",
        "                n=1,              # Solo una respuesta\n",
        "                stop=None         # No se necesitan stops específicos aquí\n",
        "            )\n",
        "            # Extraer la respuesta\n",
        "            predicted_label = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Validar si la respuesta es una de las etiquetas esperadas\n",
        "            if predicted_label in labels:\n",
        "                return predicted_label\n",
        "            else:\n",
        "                print(f\"  Advertencia: GPT devolvió una etiqueta inesperada: '{predicted_label}'. Se devolverá None.\")\n",
        "                return None # O manejar de otra forma (ej., asignar clase 'Neutral' por defecto)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error al llamar a la API de OpenAI: {e}\")\n",
        "            return None # Manejo de errores\n",
        "\n",
        "    print(f\"\\nFunción para clasificar con {GPT_MODEL} definida.\")\n",
        "\n",
        "    # --- 3.2 Evaluar GenAI en el Conjunto de Prueba ---\n",
        "    print(f\"\\nEvaluando {GPT_MODEL} en el CONJUNTO DE PRUEBA...\")\n",
        "    # Usamos el MISMO test set que para el modelo fine-tuned para una comparación justa.\n",
        "    test_texts_genai = split_datasets[\"test\"][\"text\"]\n",
        "    true_labels_genai_text = [id2label[l] for l in split_datasets[\"test\"][\"label\"]] # Etiquetas verdaderas en formato texto\n",
        "    predicted_labels_genai = []\n",
        "\n",
        "    for i, text in enumerate(test_texts_genai):\n",
        "        print(f\"  Procesando ejemplo {i+1}/{len(test_texts_genai)}...\", end='\\r')\n",
        "        predicted = classify_with_gpt(text)\n",
        "        predicted_labels_genai.append(predicted)\n",
        "        # Pequeña pausa para evitar rate limits (ajustar si es necesario)\n",
        "        # time.sleep(0.5)\n",
        "    print(\"\\nProcesamiento GenAI completado.\")\n",
        "\n",
        "    # Filtrar resultados None (donde la API falló o dio formato incorrecto)\n",
        "    valid_indices = [i for i, pred in enumerate(predicted_labels_genai) if pred is not None]\n",
        "    valid_true_labels = [true_labels_genai_text[i] for i in valid_indices]\n",
        "    valid_predicted_labels = [predicted_labels_genai[i] for i in valid_indices]\n",
        "\n",
        "    if len(valid_predicted_labels) > 0:\n",
        "        # Calcular métricas para GenAI\n",
        "        accuracy_genai = accuracy_score(valid_true_labels, valid_predicted_labels)\n",
        "        # Asegúrate de que las etiquetas usadas en f1_score sean consistentes (todas las posibles o las presentes)\n",
        "        f1_weighted_genai = f1_score(valid_true_labels, valid_predicted_labels, average=\"weighted\", labels=labels, zero_division=0)\n",
        "        f1_macro_genai = f1_score(valid_true_labels, valid_predicted_labels, average=\"macro\", labels=labels, zero_division=0)\n",
        "\n",
        "        print(\"\\nMétricas en el Conjunto de Prueba (GenAI - GPT):\")\n",
        "        print(f\"  accuracy: {accuracy_genai:.4f}\")\n",
        "        print(f\"  f1_weighted: {f1_weighted_genai:.4f}\")\n",
        "        print(f\"  f1_macro: {f1_macro_genai:.4f}\")\n",
        "        print(f\"  (Calculado sobre {len(valid_predicted_labels)} de {len(test_texts_genai)} ejemplos válidos)\")\n",
        "    else:\n",
        "        print(\"\\nNo se pudieron obtener predicciones válidas de GenAI para calcular métricas.\")\n",
        "\n",
        "    # --- 3.3 Probar Predicción con GenAI ---\n",
        "    print(\"\\nProbando predicción con GenAI:\")\n",
        "    print(f\"Texto: '{test_sentence_1}' -> Tono Predicho (GPT): {classify_with_gpt(test_sentence_1)}\")\n",
        "    print(f\"Texto: '{test_sentence_2}' -> Tono Predicho (GPT): {classify_with_gpt(test_sentence_2)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSaltando la Parte 3 debido a problemas con la API Key de OpenAI.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# **PARTE 4: COMPARACIÓN Y DISCUSIÓN FINAL**\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"\\n=== PARTE 4: Comparación y Discusión Final ===\")\n",
        "\n",
        "print(\"\\nResumen de Métricas en el Conjunto de Prueba:\")\n",
        "print(\"---------------------------------------------\")\n",
        "print(\"| Método                 | Accuracy | F1 Weighted | F1 Macro |\")\n",
        "print(\"|------------------------|----------|-------------|----------|\")\n",
        "# Imprimir métricas del modelo fine-tuned (ajustando nombres de claves si es necesario)\n",
        "ft_acc = test_metrics.get('eval_accuracy', float('nan'))\n",
        "ft_f1w = test_metrics.get('eval_f1_weighted', float('nan'))\n",
        "ft_f1m = test_metrics.get('eval_f1_macro', float('nan'))\n",
        "print(f\"| Fine-tuned Transformer | {ft_acc:^8.4f} | {ft_f1w:^11.4f} | {ft_f1m:^8.4f} |\")\n",
        "\n",
        "# Imprimir métricas de GenAI si se ejecutó\n",
        "if use_openai and len(valid_predicted_labels) > 0:\n",
        "    print(f\"| GenAI (GPT-3.5 Turbo)  | {accuracy_genai:^8.4f} | {f1_weighted_genai:^11.4f} | {f1_macro_genai:^8.4f} |\")\n",
        "else:\n",
        "    print(\"| GenAI (GPT-3.5 Turbo)  |   N/A    |     N/A     |   N/A    |\")\n",
        "print(\"---------------------------------------------\")\n",
        "\n",
        "print(\"\\nDiscusión basada en el texto de Dell (Sec 7) y este ejemplo:\")\n",
        "print(\"1.  **Rendimiento:** En este ejemplo (¡con datos simulados muy pequeños!), los resultados variarán. En la práctica, como señala Dell, un clasificador fine-tuneado con datos de alta calidad específicos del dominio *suele* igualar o superar a GenAI zero-shot/few-shot, especialmente para tareas con matices o domain shift (ej. textos históricos). GenAI puede funcionar bien para tareas directas y dominios cercanos a sus datos de preentrenamiento (web moderno).\")\n",
        "print(\"2.  **Costo de Desarrollo vs. Inferencia:**\")\n",
        "print(\"    - **Fine-tuning:** Requiere un esfuerzo inicial mayor (recolección/etiquetado de datos, código de entrenamiento, ajuste de hiperparámetros). Sin embargo, una vez entrenado, la inferencia (hacer predicciones) es muy barata y rápida, especialmente con modelos base eficientes (como DistilBERT o ALBERT).\")\n",
        "print(\"    - **GenAI:** El costo inicial de desarrollo es bajo (principalmente prompt engineering). Sin embargo, el costo de inferencia puede ser significativo si se procesan grandes volúmenes de texto, ya que cada clasificación requiere una llamada a la API (potencialmente costosa). Los costos y la latencia dependen del proveedor y modelo.\")\n",
        "print(\"3.  **Control y Reproducibilidad:**\")\n",
        "print(\"    - **Fine-tuning:** Ofrece mayor control. Puedes inspeccionar el modelo, reentrenarlo con datos específicos para corregir errores, y el modelo entrenado es tuyo, asegurando reproducibilidad si guardas el modelo y el código.\")\n",
        "print(\"    - **GenAI:** Es más una 'caja negra'. Dependes de la API del proveedor, que puede cambiar sin previo aviso, afectando la reproducibilidad y el rendimiento. Tienes menos control directo sobre el comportamiento del modelo más allá del prompt.\")\n",
        "print(\"4.  **Facilidad de Uso:**\")\n",
        "print(\"    - **Fine-tuning:** Requiere conocimientos técnicos de ML/DL (aunque librerías como Hugging Face simplifican mucho).\")\n",
        "print(\"    - **GenAI:** Más accesible para usuarios sin experiencia en ML, la barrera principal es el prompt engineering y el manejo de la API.\")\n",
        "print(\"5.  **Adaptabilidad al Dominio:** Fine-tuning permite adaptar el modelo específicamente a tu dominio (ej. lenguaje económico particular, textos históricos), lo cual es crucial si difiere mucho de los datos de preentrenamiento de GenAI (Sec 4, Domain Shift).\")\n",
        "\n",
        "print(\"\\n**Conclusión del Ejemplo:** Ambos enfoques tienen ventajas y desventajas. La elección depende de los recursos disponibles (tiempo, presupuesto, datos etiquetados, experiencia técnica), la escala del problema, los requisitos de rendimiento y la necesidad de control/reproducibilidad, como se discute en la Figura 1 del artículo de Dell.\")\n",
        "print(\"\\n**¡RECORDATORIO FINAL!:** Este fue un ejemplo con datos simulados. ¡Un proyecto real requiere un esfuerzo considerable en la obtención y etiquetado de datos de calidad!\")"
      ]
    }
  ]
}